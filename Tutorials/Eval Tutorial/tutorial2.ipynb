{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from phoenix.evals import (\n",
    "    TOOL_CALLING_PROMPT_TEMPLATE, \n",
    "    llm_classify,\n",
    "    OpenAIModel\n",
    ")\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "from openinference.instrumentation import suppress_tracing\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenTelemetry Tracing Details\n",
      "|  Phoenix Project: evaluating-agent\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: http://localhost:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from init_phoenix import init_phoenix\n",
    "PROJECT_NAME = \"evaluating-agent\"\n",
    "\n",
    "client, tool_calling_client, tracer = init_phoenix(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial1_code import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(client, tool_calling_client, tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main span with messages: [{'role': 'user', 'content': 'What was the most popular product SKU?'}]\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the most popular product SKU?'}]\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  17%|█▋        | 1/6 [00:07<00:38,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main span completed with return value: code=None observed_trends=None natural_language_response='The most popular product SKU was **6200700**, with a total quantity sold of **52,262** units.'\n",
      "Starting main span with messages: [{'role': 'user', 'content': 'What was the total revenue across all stores?'}]\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the total revenue across all stores?'}]\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  33%|███▎      | 2/6 [00:12<00:24,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main span completed with return value: code=None observed_trends=None natural_language_response='The total revenue across all stores was approximately $13,272,640.'\n",
      "Starting main span with messages: [{'role': 'user', 'content': 'Which store had the highest sales volume?'}]\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Which store had the highest sales volume?'}]\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  50%|█████     | 3/6 [00:19<00:18,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main span completed with return value: code=None observed_trends=None natural_language_response='The store with the highest sales volume is Store Number 2970, with total sales of approximately 836,341.33.'\n",
      "Starting main span with messages: [{'role': 'user', 'content': 'Create a bar chart showing total sales by store'}]\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Create a bar chart showing total sales by store'}]\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "generate_visualization\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  67%|██████▋   | 4/6 [00:50<00:32, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main span completed with return value: code=\"import pandas as pd\\nimport matplotlib.pyplot as plt\\n\\n# Data\\ndata = {'Store_Number': [2860, 2420, 770, 3520, 660, 3410, 990, 1210, 330, 2750, 1980, 1760, 4730, 4070, 3080, 2090, 1320, 2640, 1540, 4840, 1100, 3300, 3190, 3630, 2310, 2200, 1870, 2970, 3740, 2530, 4400, 880, 1650, 4180, 550],\\n        'Total_Sales': [132320.519487, 406715.767402, 292968.918642, 145701.079372, 343594.978075, 410567.848126, 378433.018639, 508393.767785, 370503.687331, 453664.808068, 242290.828499, 350747.617798, 239711.708869, 322307.968330, 495458.238811, 309996.247965, 592832.067579, 308990.318559, 427777.427815, 389056.668316, 497509.528013, 619660.167018, 335035.018792, 405034.547846, 412579.388504, 361173.288199, 401070.997685, 836341.327191, 359729.808228, 324046.518720, 95745.620250, 420302.088397, 580443.007953, 272208.118542, 229727.498752]}\\n\\n# Create DataFrame\\ndf = pd.DataFrame(data)\\n\\n# Plot\\nplt.figure(figsize=(12, 6))\\nplt.bar(df['Store_Number'], df['Total_Sales'], color='skyblue')\\nplt.title('Total Sales by Store')\\nplt.xlabel('Store_Number')\\nplt.ylabel('Total_Sales')\\nplt.xticks(rotation=45)\\nplt.grid(axis='y')\\nplt.show()\" observed_trends='The bar chart clearly illustrates the total sales across different stores, showing variations in sales levels. Some stores have significantly higher total sales compared to others, indicating potential differences in performance or market conditions across locations. Notably, the store with the highest total sales stands out, which may warrant further analysis to understand its factors of success.' natural_language_response='I have created a bar chart showing the total sales by store. The chart illustrates the sales figures for each store clearly, allowing for easy comparison. You can see the distinct sales levels across different store numbers. \\n\\nIf you need any further modifications or analysis, feel free to ask!'\n",
      "Starting main span with messages: [{'role': 'user', 'content': 'What percentage of items were sold on promotion?'}]\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What percentage of items were sold on promotion?'}]\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions:  83%|████████▎ | 5/6 [01:00<00:14, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main span completed with return value: code=None observed_trends='Approximately 2.63% of items were sold on promotion.' natural_language_response='Approximately 2.63% of items were sold on promotion.'\n",
      "Starting main span with messages: [{'role': 'user', 'content': 'What was the average transaction value?'}]\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the average transaction value?'}]\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: True\n",
      "Starting tool calls span\n",
      "lookup_sales_data\n",
      "Starting router call span\n",
      "Making router call to OpenAI\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing questions: 100%|██████████| 6/6 [01:05<00:00, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main span completed with return value: code=None observed_trends=None natural_language_response='The average transaction value was approximately $19.02.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\",\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"What percentage of items were sold on promotion?\",\n",
    "    \"What was the average transaction value?\"\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    try:\n",
    "        ret = agent.start_main_span([{\"role\": \"user\", \"content\": question}])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
